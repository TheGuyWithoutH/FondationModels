{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf7f397-467f-4f24-a18f-4143eca2c3ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CS 503 Foundation Models: Part 1 - nanoGPT\n",
    "\n",
    "We build nano4M in three parts:\n",
    "1) In this first part, we start by implementing the necessary building blocks to construct an autoregressive Transformer, like GPT.\n",
    "2) In part 2, we will build a masked model in the style of MaskGIT. \n",
    "3) In part 3, we will build a simple 4M-like multimodal model.\n",
    "\n",
    "#### Goals:\n",
    "\n",
    "The goal of this first part is to familiarize yourself with the following topics:\n",
    "- Causal attention\n",
    "- Transformer decoder-only (e.g. GPT, LLaMA, ...) models\n",
    "- Basic tokenization\n",
    "- Basic positional encodings\n",
    "- Autoregressive modelling on text and images\n",
    "- Autoregressive inference\n",
    "\n",
    "This notebook should give you a solid foundation of working with autoregressive Transformer models and get you \"thinking with tokens\".\n",
    "\n",
    "If you want to know more about these topics, please see some of the reading material in the lectures and at the bottom of this notebook, and feel free to ask the TAs.\n",
    "\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Your task is to fill in the missing code in the accompanying codebase (highlighted by `???`), run the training loops and evaluate the trained models with this notebook.\n",
    "- Submit the notebook with all cells executed.\n",
    "- The notebooks are individual homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7ba70f-25dd-4bdb-953b-d5e4547569ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4530be-c761-4957-b8f9-1bca7a35f761",
   "metadata": {},
   "source": [
    "### 1.1 Dependencies and environment\n",
    "\n",
    "The notebook should be run on one GPU, while the actual training requires 1-2 GPUs depending on the config.\n",
    "\n",
    "The required packages for training are specified in `pyproject.toml`. \n",
    "We provide a convenience script, `setup_env.sh`, which creates a `nanofm` environment and Jupyter Kernel, and installs the requirements.\n",
    "\n",
    "After running `bash setup_env.sh`, you can activate the environment with `source activate nanofm`. Similarly, you will have access to the `nano4M kernel (nanofm)` in your Jupyter notebooks for executing the following cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92056502-0af7-49eb-bb35-84e3f55a23ed",
   "metadata": {},
   "source": [
    "### 1.2 Codebase overview\n",
    "\n",
    "This nano4M codebase is a heavily simplified version of the original 4M codebase. We will use it to implement nano versions of [GPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) for autoregressive text and image generation, [MaskGIT](https://masked-generative-image-transformer.github.io/) for masked text and image generation, and [4M](https://4m.epfl.ch/) for multimodal generation.\n",
    "\n",
    "The codebase is structured in the following manner:\n",
    "\n",
    "- `cfgs/`: Configs specifying exactly what model to train on what data, for how long, etc.\n",
    "- `nanofm`: \n",
    "    - `data`: Contains various data loaders, e.g. for text, vision, and multimodal datasets.\n",
    "    - `modeling`: Contains often used modeling utils, such as Transformer layer definitions.\n",
    "    - `models`: Specific instantiations of models that define the model, forward pass, loss, and generation loop.\n",
    "    - `utils`: Various helper utils related to training, checkpointing, etc.\n",
    "- `notebooks`: Check this directory for the instructions for parts 1-3. You will need to submit these notebooks.\n",
    "- `run_training.py`: This file contains the main training and evaluation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79d566-c02d-4dba-a34c-70cc1600abb6",
   "metadata": {},
   "source": [
    "### 1.3 Weights & Biases setup\n",
    "\n",
    "When training models, it's crucial to log training and validation metrics, as well as stats about the model training that can help to check the health and efficiency of a run.\n",
    "For example, we will log train and validation losses to check that the model is converging properly and is not not overfitting to the train set.\n",
    "Besides that, we log the average norm of the gradient updates to check that it is not exploding, as a sign of the health of the training run.\n",
    "\n",
    "For all of this, we will use Weights & Biases, which also automatically logs system stats like GPU utilization, memory usage, etc.\n",
    "Please visit https://wandb.ai/ and create an account in case you don't have one already. \n",
    "\n",
    "In https://wandb.ai/settings you should see an API key. If not, please create a new one. \n",
    "Copy this key and use it to [log in](https://docs.wandb.ai/ref/cli/wandb-login/) by calling `wandb login <KEY>`. You should see the message `wandb: W&B API key is configured`.\n",
    "You may alternatively log in to wandb by adding it as an environment variable: `export WANDB_API_KEY=<KEY>`.\n",
    "\n",
    "Now, when running training jobs on a new node, make sure to log in before starting the training, and remember to fill the `wandb_entity` entry in the configs with your wandb user name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3365eba-118f-4f63-96fe-1c9904fbc050",
   "metadata": {},
   "source": [
    "### 1.3 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38061c15-c053-431a-9265-536b778fe5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch path to root of project\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "current_folder = globals()['_dh'][0]\n",
    "os.chdir(os.path.dirname(os.path.abspath(current_folder)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef049477-3435-4543-bef7-acfaea93c2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/balducci/miniconda3/envs/nanofm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f1a0571bd30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from nanofm.utils.checkpoint import load_model_from_safetensors\n",
    "from nanofm.data.vision.tokenized_mnist import create_tokenized_mnist_dataloader, detokenize_MNIST\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77511210-e44d-4ec6-8184-984b44db9ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 2 Training nanoGPT on TinyStories for text generation\n",
    "\n",
    "In this exercise, we will implement a simplified autoregressive Transformer, similar to nanoGPT. \n",
    "We will train it on [TinyStories](https://arxiv.org/abs/2305.07759), a synthetically generated dataset of somewhat simple children's book stories. That focus allows us to train relatively small models that can generate coherent text and demonstrate some basic world knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eba328-3fb2-4d47-a950-204d8df454ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Overview and tasks\n",
    "\n",
    "To implement nanoGPT, we ask you to complete the subsections below by directly filling in the missing lines in the code base.\n",
    "\n",
    "Hint: After completing the implementation of nanoGPT, in case you are still debugging, you may want to run the image generation examples in section 3 first. They are significantly faster to train and may facilitate faster debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6074f1-639a-44f8-9e3f-067361272b70",
   "metadata": {},
   "source": [
    "#### 2.1.1 MLP layer (10 points)\n",
    "\n",
    "In `nanofm.modeling.transformer_layers.Mlp`, implement the following two-layer Perceptron:\n",
    "\n",
    "$$ \\text{MLP}(X) = \\text{GeLU}(X W_1^T + b_1) W_2^T + b_2 $$\n",
    "\n",
    "Here, $\\text{GeLU}$ denotes the [GeLU activation function](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html). \n",
    "The first linear layer projects x from dimension `in_features` to `hidden_features`, while the second projects it back to `out_features`.\n",
    "Commonly, the bias terms are disabled. Make sure to take into account the `bias` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d429c-2ae2-4b45-baad-bfc91406d6a4",
   "metadata": {},
   "source": [
    "#### 2.1.2 (Masked) self-attention layer (20 points)\n",
    "\n",
    "Next, we ask you to implement a layer that performs (optionally masked) multi-headed self-attention in `nanofm.modeling.transformer_layers.Attention`.\n",
    "\n",
    "Remember the scaled dot-product attention formula:\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}_\\text{row} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V $$\n",
    "\n",
    "The queries $Q$, the keys $K$, and the values $V$ are all linear projections of $X$:\n",
    "\n",
    "$$ Q(X) = X W_Q^T $$\n",
    "$$ K(X) = X W_K^T $$\n",
    "$$ V(X) = X W_V^T $$\n",
    "\n",
    "The scaling factor $d_k$ is the dimensionality of the keys $K$, i.e. `dim // num_heads`.\n",
    "\n",
    "The attention is performed on `num_heads` heads in parallel (don't use a for loop) in `head_dim = dim // num_heads`-dimensional subspaces and the results are concatenated along the feature dimension.\n",
    "\n",
    "In addition, we want to enable masking of the attention matrix, e.g. for implementing a Transformer decoder.\n",
    "For this, the forward function takes an additional `mask` argument, specifying where to zero-out the attention matrix.\n",
    "In practice, this is implemented by replacing the values of the attention matrix (pre softmax) to minus infinity wherever we don't want any attention flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b50df-d7f8-42af-a49d-bdd1c38aa922",
   "metadata": {},
   "source": [
    "#### 2.1.3 Transformer block (10 points)\n",
    "\n",
    "Next, implement a Transformer block in `nanofm.modeling.transformer_layers.Block`. It is defined as:\n",
    "\n",
    "$$ X_a = X + \\text{Attention}(\\text{LN}(X)) $$\n",
    "$$ X_b = X_a + \\text{MLP}(\\text{LN}(X_a)) $$\n",
    "\n",
    "Here, $\\text{LN}$ denotes (two separate) LayerNorm layers.\n",
    "\n",
    "Don't forget to pass the optional mask to the self-attention layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b2a47-bd05-49b7-a2a9-858ae9e39bc6",
   "metadata": {},
   "source": [
    "#### 2.1.4 Assembling the blocks into a Transformer tunk (10 points)\n",
    "\n",
    "Now we have all the building blocks to create a Transformer trunk! \n",
    "\n",
    "In `nanofm.modeling.transformer_layers.TransformerTrunk`, create an `torch.nn.ModuleList` containing multiple Transformer blocks, and in the forward pass call them one after another.\n",
    "Again, make sure to pass the mask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184c029-93e6-48a6-ae2a-2eb5421e598e",
   "metadata": {},
   "source": [
    "#### 2.1.5 Initialize nanoGPT, implement the forward function, and loss (20 points)\n",
    "\n",
    "Finally, we can use this Transformer trunk to build a nanoGPT model, which we implement in `nanofm.models.gpt.GPT`.\n",
    "It consists of a few operations executed in series. Initialize the following modules in the constructor:\n",
    "1. The discrete input tokens are embedded with an `nn.Embedding` layer. Initialize `self.input_embedding` accordingly, taking into account the vocabulary size.\n",
    "2. On top of that, we add learnable positional embeddings. Initialize `self.positional_embedding` as an `nn.Parameter` containing a randomly initialized Tensor of shape (`max_seq_len`, `dim`).\n",
    "3. This then gets passed to a Transformer trunk. Initialize `self.trunk` with the trunk you just implemented.\n",
    "4. Finally we project the trunk output through a LayerNorm and output projection that maps the elements from the Transformer dimension to the vocabulary size (as a one-hot vector per token). Initialize `self.out_norm` and `self.to_logits`. The bias term for `self.to_logits` should always be set to False.\n",
    "\n",
    "Next, let's implement the `forward_model` function:\n",
    "1. Pass the input tokens through the embedding, add the positional embedding (make sure to account for the length of the inputs!), pass it through the Transformer trunk, output normalization, and output projection.\n",
    "2. When calling the Transformer trunk, make sure to pass a causal attention mask of shape (1, L, L), where L is the sequence length. The mask is of boolean type, and wherever it is False the attention is masked-out (i.e. set to -infinity), and otherwise it is left untouched. Remember the shape of the attention mask.\n",
    "\n",
    "Finally, we need to compute the cross-entropy loss between the logits and the ground-truth targets. Please complete the `compute_ce_loss` function accordingly, and take into account the padding token. We do not want to compute a loss on those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f29901-8554-4cf8-b1b2-d10e5b11cc5c",
   "metadata": {},
   "source": [
    "#### 2.1.6 Write the generation loop (20 points)\n",
    "\n",
    "Now we could theoretically train a model, but it would be of no use without a `generate` function. \n",
    "In this function you will run the model in a loop:\n",
    "1. Given the context so far, run the forward function to get the logits. \n",
    "2. Extract the logits of only the last token (it's the next token prediction we care about). \n",
    "3. Sample from the probability distribution specified by the last token logits. You can use the helper function `nanofm.utils.sampling.sample_tokens` for that. Make sure to pass the temperature, top-k, and top-p arguments.\n",
    "4. Concatenate the predicted next token with the context. This will be your new context for the next round. \n",
    "5. Respect the halting conditions: A) If you reach the maximum sequence length, stop generating. Take into account the length of the context so far! B) (Optional) In case you generate an `[EOS]` token corresponding to the end-of-sequence, stop generating early.\n",
    "6. Finally after doing the entire loop and halting, return the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b6865-bfc5-4b6e-8bcd-5b242d582b0e",
   "metadata": {},
   "source": [
    "#### 2.1.7 Text tokenizer overview\n",
    "\n",
    "For this part, you will not need to implement anything. This is to familiarize you with the text tokenizer we use.\n",
    "A text tokenizer's job is to take any text and turn it into a sequence of integers (i.e. encode it) that we can predict autoregressively, and to then turn that original or a predicted sequence back into text.\n",
    "Text tokenizers come in many shapes and forms, but commonly they turn subwords into a unique token, using a vocabulary of a pre-determined size. \n",
    "\n",
    "For nanoGPT, we will use the [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) tokenizer. It has a base vocabulary size of 50257, but we will add three more special tokens to it:\n",
    "- `[PAD]`: Not all samples have the same length, but in order to batch them we need to equalize their lenght. The simplest way to do this is to pad the sequences up to some maximum sequence length, e.g. 256 tokens, using special `[PAD]` tokens. During training, we do not compute a loss on these tokens.\n",
    "- `[SOS]`: The first token could just be the first token of the text sequence, but we want to be able to perform *unconditional* generation, i.e. we want to be able to generate the entire sequence and not have to give it a first token to start generating. The start-of-sequence token `[SOS]` serves as that marker. We prepend it to every sequence.\n",
    "- `[EOS]`: Given that not all samples have the same length, we need a way for the model to indicate that it is done generating the next tokens. The end-of-sequence `[EOS]` token serves that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07c5fb8-88b2-40a8-ab50-7a7ef878e75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[SOS]', 'eos_token': '[EOS]', 'unk_token': '<|endoftext|>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50257: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50258: AddedToken(\"[SOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50259: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the GPT-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", trust_remote_code=True)\n",
    "\n",
    "# Add padding, start-of-sequence, and end-of-sequence tokens\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '[SOS]',\n",
    "    'eos_token': '[EOS]',\n",
    "})\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[SOS] $A [EOS]\",\n",
    "    special_tokens=[('[EOS]', tokenizer.eos_token_id), ('[SOS]', tokenizer.bos_token_id)],\n",
    ")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38d476-9aad-42b1-a28b-8b486372e6de",
   "metadata": {},
   "source": [
    "Let's see what happens when we encode sentences of variable length. Remember, we wrap the texts with an `[SOS]` (index 50258) and `[EOS]` (index 50259) token, and pad to the maximum sequence length with a `[PAD]` (index 50257) token. If a text is longer than the specified maximum sequence length, we truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94471d2-edca-4ed2-8009-eee809689df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258,  1212,   318,   281,  1672,    13, 50259, 50257, 50257, 50257],\n",
       "        [50258,  7454,  2402,   257,   640,   612,   373,   257,  2068, 50259]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'This is an example.',\n",
    "    'Once upon a time there was a quick brown fox.',\n",
    "]\n",
    "tokens = tokenizer(texts, max_length=10, padding='max_length', truncation=True, return_tensors='pt')['input_ids']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2644e6-cf9d-44af-8cdc-ae6adc4edf61",
   "metadata": {},
   "source": [
    "We can use the same tokenizer to turn the sequences back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468482ba-b525-416d-8f4f-6335240f837b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOS]This is an example.[EOS][PAD][PAD][PAD]\n",
      "[SOS]Once upon a time there was a quick[EOS]\n"
     ]
    }
   ],
   "source": [
    "for token_seq in tokens:\n",
    "    print(tokenizer.decode(token_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a75525-3ab5-4d39-99ad-99d5b6105ea0",
   "metadata": {},
   "source": [
    "Let's define a helper function to filter out the special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47251f7-7c89-4c6d-b827-2ee4f9c31bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, text_tokenizer):\n",
    "    \"\"\" Helper function to turn token sequences back to well-formatted text. \"\"\"\n",
    "    decoded = text_tokenizer.decode(token_ids)\n",
    "    # Remove [SOS], [EOS], and [PAD] tokens along with surrounding horizontal whitespace only.\n",
    "    decoded = re.sub(r'[ \\t]*\\[(SOS|EOS|PAD)\\][ \\t]*', ' ', decoded)\n",
    "    # Collapse extra horizontal spaces in each line without touching newline characters.\n",
    "    decoded = '\\n'.join([re.sub(r'[ \\t]+', ' ', line).strip() for line in decoded.splitlines()])\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "513e5fce-30e9-441f-9343-519c95a5bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example.\n",
      "Once upon a time there was a quick\n"
     ]
    }
   ],
   "source": [
    "for token_seq in tokens:\n",
    "    print(token_ids_to_text(token_seq, text_tokenizer=tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99c198-7515-43fc-9c26-3f08388ab392",
   "metadata": {},
   "source": [
    "### 2.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoGPT/tinystories_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 2xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 run_training.py --config cfgs/nanoGPT/tinystories_d8w512.yaml\n",
    "```\n",
    "\n",
    "During training the script saves intermediate checkpoints at regular intervals. In case your training crashes, the training automatically resumes from that last checkpoint.\n",
    "In case you don't want to resume training and instead start over from scratch, please either delete the checkpoint directory of that run (e.g. in `./outputs/nanoGPT/tinystories_d8w512/`) or rename it.\n",
    "\n",
    "This training should take over one hour. You should reach a final validation loss around 1.3, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoGPT_tinystories.png\" alt=\"nanoGPT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94b2be-6984-4c5d-8f70-e0822b153b01",
   "metadata": {},
   "source": [
    "### 2.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nanoGPT TinyStories loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbab3a-bb76-4e5d-bc78-b74bf6063dd7",
   "metadata": {},
   "source": [
    "### 2.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe67e53-b62f-4664-98b3-e6a8471c82e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './outputs/nanoGPT/tinystories_d8w512/checkpoint-final.safetensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputs/nanoGPT/tinystories_d8w512/checkpoint-final.safetensors\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_safetensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mget_num_params()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mM parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Users/Ugo/Documents/GitHub/FondationModels/nanofm/utils/checkpoint.py:203\u001b[0m, in \u001b[0;36mload_model_from_safetensors\u001b[0;34m(ckpt_path, device, to_eval)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model_from_safetensors\u001b[39m(\n\u001b[1;32m    199\u001b[0m     ckpt_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    200\u001b[0m     device: Optional[Union[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m     to_eval: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    202\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[0;32m--> 203\u001b[0m     ckpt, config \u001b[38;5;241m=\u001b[39m \u001b[43mload_safetensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     model \u001b[38;5;241m=\u001b[39m instantiate(config)\n\u001b[1;32m    205\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(ckpt)\n",
      "File \u001b[0;32m/mnt/c/Users/Ugo/Documents/GitHub/FondationModels/nanofm/utils/checkpoint.py:180\u001b[0m, in \u001b[0;36mload_safetensors\u001b[0;34m(safetensors_path, return_metadata)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_safetensors\u001b[39m(safetensors_path, return_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msafetensors_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    181\u001b[0m         data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    183\u001b[0m     tensors \u001b[38;5;241m=\u001b[39m load_st(data)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './outputs/nanoGPT/tinystories_d8w512/checkpoint-final.safetensors'"
     ]
    }
   ],
   "source": [
    "ckpt_path = './outputs/nanoGPT/tinystories_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78151077-58e0-4499-85f1-92122d6ffd25",
   "metadata": {},
   "source": [
    "Let's generate some random (unconditional) stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d40c05a6-fd08-4fe9-bdbe-86f61a1d6c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a poor family who lived by the edge of a forest. One day, the mother felt something strange. She looked around her edge and saw a crack in the ground. She grabbed a stick and stepped back. Then, she remembered how her grandmother used to send lightening rain. She thought to herself, \"I need to do it myself!\" So, she knelt back and watched. The rain got heavier and heavier and soon the crack was gone. The mother was so relieved, she hugged her man from last far away.\n",
      "\n",
      "From then on, the mother listened carefully to Joy every time she went out into the forest. She remembered the rain, the crack, and the rain that led to time's over.\n",
      "\n",
      "Every time the rain came, even when the bucket in the ground was now filled with water and no rainy day. She was always happy to visit the forest, hoping to see the light come in again soon.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Once upon a time, there was a little girl named Lily. She had a bed that was very cold and dark. Lily's mommy said, \"Lily, your bed is too bed. You need to go to sleep, but I am scared to come back outside.\"\n",
      "\n",
      "Lily said, \"I regret going outside and I can't sleep. I am lost and I miss my mommy.\"\n",
      "\n",
      "Mommy hugged Lily and said, \"Don't worry, we will find you soon. And maybe we can read happy stories together.\"\n",
      "\n",
      "Lily felt happy and safe in her bed. She said, \"Thank you, mommy. You promised me we will always be together.\" And they both smiled and snuggled together, feeling Kate by her side until it was time for bed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Tom and Anna are playing with their toys in the living room. Tom has a big truck and Anna has a doll. They are having fun, but they also want to try something new.\n",
      "\n",
      "\"Can I test my truck?\" Anna asks Tom.\n",
      "\n",
      "\"OK, here you go,\" Tom says to Anna. He gives her the truck andMath book. Anna looks at the book and nods. She likes math. It is fun and makes her move.\n",
      "\n",
      "\"Can I test your truck?\" Anna asks Tom again.\n",
      "\n",
      "\"Sure, here you go,\" Tom says. He gives her the truck and Sheep and he shows her how it works. She smiles and says thank you.\n",
      "\n",
      "Anna takes the doll and makes a dress out of the bear. She shows Tom how she keeps the bear as a ghost. Tom laughs and says, \"That's cool! But can I test your doll too?\"\n",
      "\n",
      "\"Sure,\" Anna says and puts the bear next to the doll. She turns the car around and makes a whoosh sound. Tom watches and says, \"Wow, that's cool!\"\n",
      "\n",
      "They have a lot of fun testing their toys. They make wise spies and bumped into each other sometimes. They laugh and smile a\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "above, there was a tall tree in the forest. But the tree couldnâ€™t stay still. It began to lean against the tree and shake!\n",
      "\n",
      "The animals from the forest were scared of the tree. They backed away, but the tree stayed put. Its big feet scared the animals, so they couldnâ€™t shake the tree.\n",
      "\n",
      "The tree kept leaning but the animals were getting closer and closer. They were scared of the tall tree, so they ran away from it.\n",
      "\n",
      "When the animals were gone, the tree stayed tall and strong, so the animals might smell the tall tree and try to make it look like it was still held that morning.\n",
      "\n",
      "The animals were relieved and they knew that no matter what happened, to stay away from tall trees!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Once upon a time there was a little squirrel who was very selfish. She had a wish to throw acorns all over the home around her. She shook her little body, scurrying towards the forest just to take as much.\n",
      "\n",
      "On her journey, she soon stumbled across a strange twisted tree. The tree was much bigger than the squirrel! At that moment, a prayed were heard from the tree. It said, \"If you want to keep your acorns safe, you must give me something in return.\"\n",
      "\n",
      "The little squirrel listened and chose to give the other acorns. She didn't have enough time to think about what to do. Suddenly, an idea jumped into her head. She decided to take all the acorns on the ground and deliver them to the tree.\n",
      "\n",
      "She was determined and pick up all the acorns, when she was done. And when the tree wasn't looking, she gave the acorns to the tree!\n",
      "\n",
      "The tree was so surprised and thanked her for the interesting advice. After that, the selfish squirrel never left the forest without a twirl. The moral of the story is to never be selfish and you will always use your smarts!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    output = model.generate(context=[tokenizer.bos_token_id], temp=1.0, top_p=0.0, top_k=0.0, eos_idx=tokenizer.eos_token_id)[0]\n",
    "    print(token_ids_to_text(output, text_tokenizer=tokenizer))\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024a84c-7727-4c17-acb5-4b415531726c",
   "metadata": {},
   "source": [
    "We don't have to generate the stories in a completely unconditional way. Let's try to probe the model and see if it learned some world knowledge. \n",
    "If we probe the model with the phrase `Daisy was hungry, so she`, we should expect a continuation talking about her getting food. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf89eced-7cfd-42ef-a0d9-26758f7c543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daisy was hungry, so she asked her mum for something to eat. Her mum gave her napkins to hold together. Daisy looked at them and smiled, she had found one.\n",
      "\n",
      "Daisy and her mum sat together on the sofa. Daisy saw a cheese in a cookie. She was so excited and clapped her hands.\n",
      "\n",
      "Daisy's mum said, \"Let's make a cheese sandwich for you to eat.â€ She handed Daisy the napkins.\n",
      "\n",
      "Daisy's dad caught a hot cheese pie. He said, \"Here's a sweet and warm snack! vanish into thin pieces for you.\" Daisy was so happy.\n",
      "\n",
      "Daisy's mum said, \"That was very generous of us. I'm glad you shower your napkin!\" Daisy smiled and said, \"Me too!\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Daisy was hungry, so she went to the kitchen. She heard her mommy's voice and knew her mommy had something special for her. Daisy went to the kitchen and saw mommy cooking something warm and squ disgusting in the kitchen. Daisy was scared, but she went to help. Mommy said, \"What do you recognize?\" Daisy smiled and said, \"when I'm done cooking, mommy!\" Mommy smiled and thanked Daisy. She showed Daisy how to make spaghetti for dinner. Daisy was so proud of herself for helping her mommy, and she realized that grown throwing things out was not so bad.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Daisy was hungry, so she went to the kitchen. She saw a plate on the floor. It was a big, round plate with cheese on it. Daisy smiled and started to toes.\n",
      "\n",
      "She touched the plate. It felt cold and fuzzy, but she liked it. She moved on it, making it bounce. She wanted to get theSometimes sit, so she could eat the cheese.\n",
      "\n",
      "Then Daisy saw some crayons next to the big plate. She papered and drew pictures and shapes on the note. She had lots of fun admiring all the pictures.\n",
      "\n",
      "Soon Daisy was done. It was time for lunch. Daisy's mom had arrived in the kitchen with her steaming, delicious sandwiches.\n",
      "\n",
      "Daisy happily ate her lunch and threw away the crumbs. She was no longer hungry, but she was satisfied after all her adventure.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Daisy was hungry, so she got something yummy. It was a tomato. It was bright yellow, with yellow spots on it. Daisy loved tomatoes. She took a bite and it was so tasty!\n",
      "\n",
      "Suddenly, Daisy felt something happiest. Daisy looked up and saw the sun was shining down. The sun made the tomato even sweeter. Daisy smiled and thought the sun was happy too.\n",
      "\n",
      "Daisy said, \"Mommy, I'm so happy. The sun is shining starts toobs like us!\"\n",
      "\n",
      "Mommy smiled. She said, \"Yes, Daisy. The sun is giving us the time to be happy.\"\n",
      "\n",
      "With that, Daisy and Mommy went back to eating. Daisy was happy too, and danced around with her new heavy tomato.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Daisy was hungry, so she asked her mum for something to eat. Her mum made her a sandwich and her brother brought some milk and a w Rory. Daisy was so happy and ate the sandwich quickly.\n",
      "\n",
      "After she finished eating, Daisy asked her mum for another animal to throw. But her mum said no, it was time for lunch and Daisy felt embarrassed. Her face was yellow and she wanted a minute to cry.\n",
      "\n",
      "\"Come on Daisy,\" said her mum, \"the food will keep you both energy\". Daisy's heart was beating very fast and she felt a bit guilty, so she decided to listen to her mum and stopped complaining.\n",
      "\n",
      "Her mum said \"Porse today is interesting! There are lots of happy animals in the ocean\". Daisy smiled and felt much better.\n",
      "\n",
      "The next time Daisy made a request, her mum reminded her to stay full and not throw her food. Daisy said \"Ok!\", and smiled. She was very happy and she never felt guilty about not having a burger before.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = tokenizer.encode('Daisy was hungry, so she')[:-1] # Encode and discard automatically added [EOS] token\n",
    "\n",
    "for _ in range(5):\n",
    "    output = model.generate(context=context, temp=1.0, top_p=0.0, top_k=0.0, eos_idx=tokenizer.eos_token_id)[0]\n",
    "    print(token_ids_to_text(output, text_tokenizer=tokenizer))\n",
    "    print('\\n' + '-'*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb687d48-e5ad-48a2-9e5b-e01099505d61",
   "metadata": {},
   "source": [
    "### 2.5 Open-ended questions (5 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary.\n",
    "\n",
    "- Q2.1: What effect does the temperature have on the generations?\n",
    "    - A2.1: [Please fill your answer here]\n",
    "- Q2.2: What about the top_k and top_p parameters?\n",
    "    - A2.2: [Please fill your answer here]\n",
    "- Q2.3: Sometimes the generations of this model are not very good. What could we do to improve it further?\n",
    "    - A2.3: [Please fill your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ff7f9-f249-4412-a3f6-e60310db279a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 3 Training nanoGPT on MNIST for conditional image generation\n",
    "\n",
    "In this part, we will train our nanoGPT on images, similar in spirit to [iGPT](https://openai.com/research/image-gpt).\n",
    "What this means is that we turn images into a sequence of discrete tokens and train a decoder-only Transformer to perform next-token-prediction.\n",
    "At inference time, we can then autoregressively generate new images from the training distribution, conditioned on the class label!\n",
    "\n",
    "If you have successfully completed the nanoGPT implementation, you can directly re-use it for this step! The only thing we change is the way we represent the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0d1209-9135-4254-929f-723b52862259",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization\n",
    "\n",
    "First, we need to turn images into sequences of discrete tokens.\n",
    "\n",
    "A common way of performing tokenization of images is to train a discrete VAE, such as [VQ-GAN](https://arxiv.org/abs/2012.09841) to autoencode images through a discrete bottleneck. This is an entire research field of its own, so we will not go further into that here.\n",
    "\n",
    "Instead, we will tokenize the images in a much simpler way:\n",
    "- First, we turn the grayscale images into black and white → each image is now a sequence of 14x14 zeros and ones. In this manner, we would already have a sequence of discrete tokens we could model, but the sequence length of 14x14=192 could be shorter.\n",
    "- To reduce the sequence length further, we divide the image into 2x2 patches and turn each of the 2x2 patterns into a unique index. If we flatten each 2x2 patch into a sequence of 4 zeros and ones, we can directly interpret them as integers between 0 and 15. This reduces the sequence length by a factor of four. Much more manageable in our toy setting!\n",
    "\n",
    "While this way of tokenizing is really quite \"toy\", it is indicative of a common problem when dealing with images and transformers: naively turning images to tokens can result in very large sequence lengths.\n",
    "\n",
    "Let's plot some examples of the tokenized images. First is the original image, then we show each token, and finally we show the corresponding token sequence. \n",
    "The first token is the class-label, and the next 49 are the image tokens in raster-scan order. Their token id was shifted by 10 to account for the additional class token (we share the vocabulary with it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c72a238c-283d-4d48-9f9b-dceb30e57729",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "data_loader = create_tokenized_mnist_dataloader(train=False, add_label_token=True, shuffle=False)\n",
    "data_dict = next(iter(data_loader))\n",
    "\n",
    "tokens = data_dict['input_ids']\n",
    "reconst = detokenize_MNIST(tokens, patch_size=2, account_for_labels=True)\n",
    "bits = rearrange(reconst, 'b (nh ph) (nw pw) -> b (nh nw) ph pw', ph=2, pw=2)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure(figsize=(5., 5.)); plt.imshow(reconst[i], cmap='Greys'); plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(5., 5.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(7, 7), axes_pad=0.1)\n",
    "    for j, img in enumerate(bits[i]):\n",
    "        grid[j].imshow(img, cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "    print('Tokens:', tokens[i], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a78d4a-6df4-4d20-8627-a393ddf9634d",
   "metadata": {},
   "source": [
    "### 3.2 Training the model\n",
    "\n",
    "We defined a training config for you in: `cfgs/nanoGPT/mnist_d8w512.yaml`. Please familiarize yourself with all parts.\n",
    "Please don't forget to replace the Weights & Bias entity with your own.\n",
    "\n",
    "On a 1xV100 node, you can start the training like:\n",
    "```\n",
    "OMP_NUM_THREADS=1 torchrun --nproc_per_node=1 run_training.py --config cfgs/nanoGPT/mnist_d8w512.yaml\n",
    "```\n",
    "\n",
    "This training should be pretty fast and only take a few minutes. You should reach a final validation loss below 0.45, and your loss curves should look something like the following:\n",
    "\n",
    "<img src=\"./assets/nanoGPT_mnist.png\" alt=\"nanoGPT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92233937-b07b-4313-8004-b17e058ca380",
   "metadata": {},
   "source": [
    "### 3.3 Show your loss curves (10 points)\n",
    "\n",
    "Screenshot your loss curves and show them here. Add the image to the `assets` directory and change the path in the markdown. You will get 10 points for reasonable loss curves (similar to the sample loss curves above).\n",
    "\n",
    "<img src=\"./assets/your_loss_curves.png\" alt=\"nanoGPT MNIST loss curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdd8d6-a5fe-46a3-ba4e-7ee872d495ff",
   "metadata": {},
   "source": [
    "### 3.4 Evaluating the model (10 points)\n",
    "\n",
    "After you completed the training, load the model with the following cell. You may need to adjust the path if you changed it.\n",
    "You will get 10 points if the outputs look reasonable (similar to the sample outputs provided below).\n",
    "\n",
    "Hint: You can also load intermediate safetensors checkpoints to check the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cede1ce4-dfff-4a99-aa8d-1761f2141c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.200128M parameters\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = './outputs/nanoGPT/mnist_d8w512/checkpoint-final.safetensors'\n",
    "model = load_model_from_safetensors(ckpt_path, device=device)\n",
    "print(f'{model.get_num_params() / 10**6}M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54f04f-d106-4cef-b23b-936a91854bd9",
   "metadata": {},
   "source": [
    "Let's plot some class-conditional generations! We seed the generation by providing the first token, whose index is equal to the number we'd like to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989eecc3-7c72-4a7f-a541-862ab263bc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 13, 13, 10, 10, 10, 10, 15,\n",
       "         10, 10, 10, 10, 10, 10, 15, 13, 12, 10, 10, 10, 10, 10, 10, 25, 10, 10,\n",
       "         10, 10, 11, 13, 18, 10, 10, 10, 10, 10, 10, 10, 10, 10]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = 5\n",
    "output = model.generate(context=[label], temp=0.7, top_p=0.0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93a41606-6524-47e4-90ea-ab0b271f0504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x70a72afba440>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "plt.imshow(reconst[0], cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0c051-cf4f-42ba-ba02-716ad3d2add2",
   "metadata": {},
   "source": [
    "Let's now generate 10 random samples for all 10 classes. Most should look quite reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ca0356c-eb7e-4cab-b242-27525d6a3b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 200 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_samples(model, temp=1.0, top_p=0.0, top_k=0.0, n_samples=10):\n",
    "    fig = plt.figure(figsize=(8., 8.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(10, n_samples), axes_pad=0.1)\n",
    "    for label in range(10):\n",
    "        for sample_idx in range(n_samples):\n",
    "            grid_idx = label * n_samples + sample_idx\n",
    "            output = model.generate(context=[label], temp=temp, top_p=top_p, top_k=top_k)\n",
    "            reconst = detokenize_MNIST(output, patch_size=2, account_for_labels=True).cpu()\n",
    "            grid[grid_idx].imshow(reconst[0], cmap='Greys', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "    \n",
    "generate_samples(model, temp=0.7, top_p=0.9, top_k=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29571b8-ebaf-4ef1-8426-4863e986fff6",
   "metadata": {},
   "source": [
    "### 3.5 Open-ended questions (5 points each)\n",
    "\n",
    "Please answer the following questions. You may use additional cells to demonstrate your answers if necessary.\n",
    "\n",
    "- Q3.1: What effect does the temperature have on the generations?\n",
    "    - A3.1: [Please fill your answer here]\n",
    "- Q3.2: What about the top_k and top_p parameters?\n",
    "    - A3.2: [Please fill your answer here]\n",
    "- Q3.3: How might we extend this to text-to-image?\n",
    "    - A3.3: [Please fill your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8908f8-c491-4519-88c3-72da701e6933",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4 Further reading\n",
    "\n",
    "Having implemented causal-attention, as well as a decoder-only Transformer, you should not have a hard time implementing cross-attention and a full encoder-decoder Transformer to train on arbitrary sequence-to-sequence tasks.\n",
    "We will explore these topics in the next weeks.\n",
    "That said, here is some further reading material should you want to dive deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbff92e-3808-41b0-a4fe-c3746b6acfdc",
   "metadata": {},
   "source": [
    "### 4.1 Papers & Blogs\n",
    "\n",
    "- [Attention Is All You Need, Vaswani et al. 2017 (Original Transformer paper)](https://arxiv.org/abs/1706.03762)\n",
    "- [Language Models are Unsupervised Multitask Learners, Radford et al. 2018 (GPT-2 paper)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "- [Language Models are Few-Shot Learners, Brown et al. 2020 (GPT-3 paper)](https://arxiv.org/abs/2005.14165)\n",
    "- [PaLM: Scaling Language Modeling with Pathways, Chrowdherry et al. 2022](https://arxiv.org/abs/2204.02311)\n",
    "- [LLaMA: Open and Efficient Foundation Language Models, Touvron et al. 2023](https://arxiv.org/abs/2302.13971)\n",
    "- [Llama 2: Open Foundation and Fine-Tuned Chat Models, Touvron et al. 2023](https://arxiv.org/abs/2307.09288)\n",
    "- [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)\n",
    "- [Scaling Laws for Neural Language Models, Kaplan et al. 2020](https://arxiv.org/abs/2001.08361)\n",
    "- [Training Compute-Optimal Large Language Models, Hoffmann et al. (Chinchilla)](https://arxiv.org/abs/2203.15556)\n",
    "- [Pixel Recurrent Neural Networks, Van den Oord et al. 2016 (PixelCNN)](https://arxiv.org/abs/1601.06759)\n",
    "- [PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications, Salimans et al. 2017](https://arxiv.org/abs/1701.05517)\n",
    "- [Zero-Shot Text-to-Image Generation, Ramesh et al. 2021 (DALL-E 1)](https://arxiv.org/abs/2102.12092)\n",
    "- [Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation, Sun et al. 2024 (Image generation with simple autoregressive models)](https://arxiv.org/abs/2406.06525)\n",
    "- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Dosovitskiy et al. 2020 (Vision Transformer paper)](https://arxiv.org/abs/2010.11929)\n",
    "- [Transformer Circuits Thread](https://transformer-circuits.pub/)\n",
    "- [The Transformer Family Version 2.0, Lilian Weng 2023](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n",
    "- [The Illustrated Transformer, Jay Alammar 2018](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ec4f0-9057-4a8c-ac92-dc9603c5012c",
   "metadata": {},
   "source": [
    "### 4.2 Code bases\n",
    "\n",
    "- [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
    "- [Llama model inference code](https://github.com/meta-llama/llama-models)\n",
    "- [LlamaGen](https://github.com/FoundationVision/LlamaGen)\n",
    "- [torchtitan](https://github.com/pytorch/torchtitan)\n",
    "- [lingua](https://github.com/facebookresearch/lingua)\n",
    "\n",
    "You might find that, in many ways, these repos are not that much different from what we implemented here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e5682-f0d7-415c-9c7c-6aae68b0fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanofm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
